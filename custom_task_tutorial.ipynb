{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d838b9ec-24b8-4307-b5e7-042ae8f0ba5d",
   "metadata": {},
   "source": [
    "# Building a custom task\n",
    "\n",
    "First, we need to import a few things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e2f39ff-8382-494d-a085-e9d69515c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "import keras.models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26180fa4-85f4-470c-ad85-ea83ac0fabbe",
   "metadata": {},
   "source": [
    "Now let's build a neural network! First we'll lay out the code, then we'll walk through it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d46729-6f6a-4400-b53b-7fb4bd8d7c26",
   "metadata": {},
   "source": [
    "## The Custom Task Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19999210-29c2-47a4-81a0-6625f173cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datarobot_drum.custom_task_interfaces import RegressionEstimatorInterface\n",
    "\n",
    "class CustomTask(RegressionEstimatorInterface):\n",
    "    def fit(self, X, y, row_weights=None, **kwargs):\n",
    "        \"\"\" This hook defines how DataRobot will train this task.\n",
    "        DataRobot runs this hook when the task is being trained inside a blueprint.\n",
    "        As an output, this hook is expected to create an artifact containing a trained object, that is then used to predict new data.\n",
    "        The input parameters are passed by DataRobot based on project and blueprint configuration.\n",
    "\n",
    "        Parameters\n",
    "        -------\n",
    "        X: pd.DataFrame\n",
    "            Training data that DataRobot passes when this task is being trained.\n",
    "        y: pd.Series\n",
    "            Project's target column.\n",
    "        row_weights: np.ndarray (optional, default = None)\n",
    "            A list of weights. DataRobot passes it in case of smart downsampling or when weights column is specified in project settings.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        CustomTask\n",
    "            returns an object instance of class CustomTask that can be used in chained method calls\n",
    "        \"\"\"\n",
    "        tf.random.set_seed(1234)\n",
    "        input_dim, output_dim = len(X.columns), 1\n",
    "\n",
    "        model = Sequential(\n",
    "            [\n",
    "                Dense(\n",
    "                    input_dim, activation=\"relu\", input_dim=input_dim, kernel_initializer=\"normal\"\n",
    "                ),\n",
    "                Dense(input_dim // 2, activation=\"relu\", kernel_initializer=\"normal\"),\n",
    "                Dense(output_dim, kernel_initializer=\"normal\"),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", \"mse\"])\n",
    "\n",
    "        callback = EarlyStopping(monitor=\"loss\", patience=3)\n",
    "        model.fit(\n",
    "            X, y, epochs=20, batch_size=8, validation_split=0.33, verbose=1, callbacks=[callback]\n",
    "        )\n",
    "\n",
    "        # Attach the model to our object for future use\n",
    "        self.estimator = model\n",
    "        return self\n",
    "\n",
    "    def save(self, artifact_directory):\n",
    "        \"\"\"\n",
    "        Serializes the object and stores it in `artifact_directory`\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        artifact_directory: str\n",
    "            Path to the directory to save the serialized artifact(s) to\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        # If your estimator is not pickle-able, you can serialize it using its native method,\n",
    "        # i.e. in this case for keras we use model.save, and then set the estimator to none\n",
    "        keras.models.save_model(self.estimator, Path(artifact_directory) / \"model.h5\")\n",
    "\n",
    "        # Helper method to handle serializing, via pickle, the CustomTask class\n",
    "        self.save_task(artifact_directory, exclude=['estimator'])\n",
    "\n",
    "        return self\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, artifact_directory):\n",
    "        \"\"\"\n",
    "        Deserializes the object stored within `artifact_directory`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cls\n",
    "            The deserialized object\n",
    "        \"\"\"\n",
    "\n",
    "        # Helper method to load the serialized CustomTask class\n",
    "        custom_task = cls.load_task(artifact_directory)\n",
    "\n",
    "        custom_task.estimator = keras.models.load_model(Path(artifact_directory) / \"model.h5\")\n",
    "\n",
    "        return custom_task\n",
    "\n",
    "    def predict(self, X, **kwargs):\n",
    "        \"\"\" This hook defines how DataRobot will use the trained object from fit() to transform new data.\n",
    "        DataRobot runs this hook when the task is used for scoring inside a blueprint.\n",
    "        As an output, this hook is expected to return the transformed data.\n",
    "        The input parameters are passed by DataRobot based on dataset and blueprint configuration.\n",
    "\n",
    "        Parameters\n",
    "        -------\n",
    "        X: pd.DataFrame\n",
    "            Data that DataRobot passes for transformation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Returns a dataframe with transformed data.\n",
    "        \"\"\"\n",
    "        # Note how the regression estimator only outputs one column, so no explicit column names are needed\n",
    "        return pd.DataFrame(data=self.estimator.predict(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9b5af-854b-4a8f-b174-6c58b7fa005e",
   "metadata": {},
   "source": [
    "There's a lot above, so don't worry about reading through it all now. The key idea is that we have several hooks, specifically fit, save, load, and predict. DataRobot will use these hooks automatically to run our custom task. You can copy the above cell directly in a custom.py file, add in an optional (but highly recommeneded) model-metadata.yaml, and then you're ready to upload this CustomTask to DataRobot! See [placeholder] to see exactly how we setup the code above into a custom task folder ready for upload.\n",
    "\n",
    "Note: The class above is an ordinary python class, so you can easily add helper methods or even import entire helper files! See [placeholder for VisualAI] for a more complex neural network that uses helper functions in a separate file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016bfbc-dc73-4f2e-a936-d6db6e4ccef7",
   "metadata": {},
   "source": [
    "Now let's actually use the class above. Since this is an ordinary python class, all we need to do is build an object and we can test it out to ensure our methods work! First, let's grab a dataset and then separate out the target column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283bc704-037a-499a-9e21-55b1404480a9",
   "metadata": {},
   "source": [
    "## Training our Custom Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8723510-6c27-4ece-8bfc-0e28921fef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tests/testdata/juniors_3_year_stats_regression.csv\")\n",
    "\n",
    "y = df['Grade 2014']\n",
    "X = df.drop(labels=['Grade 2014'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e4c8c8-110d-464d-834b-c6560d230625",
   "metadata": {},
   "source": [
    "Now let's train our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b4a1f57-c912-4ced-9bbe-fa7ab1cbc2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = CustomTask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82989943-c38f-403f-bc08-27b8370d59b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 09:48:32.035510: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-01 09:48:32.193202: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "124/124 [==============================] - 1s 2ms/step - loss: 243.6316 - mae: 11.2293 - mse: 243.6316 - val_loss: 127.8057 - val_mae: 7.8969 - val_mse: 127.8057\n",
      "Epoch 2/20\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 119.6218 - mae: 7.3784 - mse: 119.6218 - val_loss: 126.4541 - val_mae: 7.1894 - val_mse: 126.4541\n",
      "Epoch 3/20\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 119.6898 - mae: 7.4064 - mse: 119.6898 - val_loss: 131.0404 - val_mae: 6.7115 - val_mse: 131.0404\n",
      "Epoch 4/20\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 118.2549 - mae: 7.2968 - mse: 118.2549 - val_loss: 126.3128 - val_mae: 8.0251 - val_mse: 126.3128\n",
      "Epoch 5/20\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 118.4696 - mae: 7.3713 - mse: 118.4696 - val_loss: 124.3396 - val_mae: 7.1751 - val_mse: 124.3396\n",
      "Epoch 6/20\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 119.1182 - mae: 7.2953 - mse: 119.1182 - val_loss: 129.7029 - val_mae: 6.6129 - val_mse: 129.7029\n",
      "Epoch 7/20\n",
      "124/124 [==============================] - 0s 1ms/step - loss: 119.7277 - mae: 7.4162 - mse: 119.7277 - val_loss: 123.8151 - val_mae: 7.6164 - val_mse: 123.8151\n"
     ]
    }
   ],
   "source": [
    "task = task.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a889669-2be2-403d-985b-f62c4a61a9ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1daaddde-581e-4e60-bff2-9a104cb5c79c",
   "metadata": {},
   "source": [
    "## Saving and Loading our Custom Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7a04a-1ad6-4b0f-8529-541907b03942",
   "metadata": {},
   "source": [
    "Saving our model is critically important. For performance reasons, DataRobot actually separates training a model with the fit function vs. making predictions with the model with the predict() function. This means that we have to save, or serialize, everything we want to use in the predict() function. One challenge is that each machine learning library may use a slightly different serialization format. For example, sklearn uses pickle to serialize, whereas the keras framework has its own model.save() and load_model. \n",
    "\n",
    "By default, the CustomTask will pickle a model. That meants for a standard sklearn model, you don't even have to write a save or load hook! The CustomTask class has a built in save and load mehtod that will create a pickle for you (you will see it in your files as drum_artifact.pkl)\n",
    "\n",
    "If you look above, you'll see we actually overrode the built in save and load methods. That's because we need to save our keras model using it's own serialization methods, in this case saving as a .h5 file. One thing to notice is that all we have to do is save off our model, typically saved in self.estimator, and then we can call the save_task helper function. This will automatically pickle the CustomTask class and exclude (i.e. set to None) any objects we pass along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "124a1e4c-6055-431f-9a82-d8e4203abab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mCustomTask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m        Serializes the object and stores it in `artifact_directory`\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Parameters\u001b[0m\n",
       "\u001b[0;34m        ----------\u001b[0m\n",
       "\u001b[0;34m        artifact_directory: str\u001b[0m\n",
       "\u001b[0;34m            Path to the directory to save the serialized artifact(s) to\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Returns\u001b[0m\n",
       "\u001b[0;34m        -------\u001b[0m\n",
       "\u001b[0;34m        self\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# If your estimator is not pickle-able, you can serialize it using its native method,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# i.e. in this case for keras we use model.save, and then set the estimator to none\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_directory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Helper method to handle serializing, via pickle, the CustomTask class\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martifact_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'estimator'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /var/folders/zn/dc_gkkq95794qfb944f3_44r0000gq/T/ipykernel_90111/775694253.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??CustomTask.save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987e1dd-f388-4096-915e-c1ed0329c791",
   "metadata": {},
   "source": [
    "Loading a custom task is simply the opposite approach. We use a helper method to read in our CustomTask object, then use the keras load_model method to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea43b8ec-80f9-4b99-8eaf-f778aba66225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CustomTask at 0x187f5c350>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.save(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5419aaa-ab0e-4bd9-bd54-918af84a35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = task.load(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ffeebdf-403e-4efb-a40c-4fc92af07c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185/185 [==============================] - 0s 561us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.767523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.769978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.650490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.763115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.636791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>28.275587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>27.721256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>32.391720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>24.609760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>32.922050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1477 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0     28.767523\n",
       "1     27.769978\n",
       "2     28.650490\n",
       "3     25.763115\n",
       "4     30.636791\n",
       "...         ...\n",
       "1472  28.275587\n",
       "1473  27.721256\n",
       "1474  32.391720\n",
       "1475  24.609760\n",
       "1476  32.922050\n",
       "\n",
       "[1477 rows x 1 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb8e275-f42a-42a4-b814-b70f1247223b",
   "metadata": {},
   "source": [
    "TODO: mention what they'll need to copy into custom.py (have a separate folder for this example so they can see the difference between notebook land and custom.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a855948-2cf5-4e0c-9bca-fac3b83d0c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datarobot.rest.RESTClientObject at 0x18168d7d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datarobot as dr\n",
    "dr.Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5dd6b0c-4ca6-409a-946a-8022329378ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datarobot_bp_workshop import Workshop\n",
    "w = Workshop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7c1074-38b2-4431-b41c-a4ea17bd5724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drum_cli_enduser",
   "language": "python",
   "name": "drum_cli_enduser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
